<!DOCTYPE html>
<html lang="en">
    <head>
        <!--meta data-->
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="Frank Regal Personal Website" />
        <meta name="keywords" content="portfolio" />
        <meta name="author" content="Frank Regal" />
        <title>Frank Regal - Porfolio</title>
        <!--meta data-->
    
        <!--favicon-img--> 
        <link rel="icon" type="image/png" href="images/fr_logo.png">
        <!--favicon-img-->
    
        <!--Google Fonts-->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Mr+Dafoe&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Mrs+Sheppards&display=swap" rel="stylesheet">
        <!--Google Fonts-->
    
        <!--main css file-->
        <link rel="stylesheet" href="css/index.css">
        <!--main css file-->
    
        <!--load java script file--> 
        <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.2.6/gsap.min.js"></script>
        <!--load java script file--> 
    </head>
    <body>
        <!--// Page Navigation ========================================================= -->
        <!--navigator-->
        <div id="navigation-bar">
            <a href="index.html" class="nav-name">FR</a>
            <ul class="navigation-links">
                <!-- <li><a href="#" data-text="" id="home-link" >Home</a></li> -->
                <li><a href="portfolio.html" data-text="" id="blog-link" >Projects</a></li>
                <li><a href="https://scholar.google.com/citations?user=A0RqAUgAAAAJ&hl=en&oi=sra" data-text="" id="contact-link" >Publications</a></li>
                <li><a href="https://1drv.ms/w/c/573fe21775d93be1/EdEFwUW9jwpAnPF5gp_8Zo0BMBXjoU_v5qz4fY5dTWffcg?e=dsuyWW" data-text="" id="cv" >Curriculum Vitae</a></li>
                <li><a href="about.html" data-text="" id="about-link" >Bio</a></li>  
            </ul>
        </div>
        <!--navigator-end-->
        <!--menubar-->
        <div class="menubar" id="menubar">
            <span class="first-span"></span>
            <span class="second-span"></span>
            <span class="third-span"></span>
        </div>
        <!--menubar-->
        <!-- Mobile Navigation Menu -->
        <div class="mobile-nav" id="mobile-nav">
            <div class="mobile-nav-content">
                <div class="mobile-nav-header">
                    <div class="nav-name-mobile">FR</div>
                    <div class="close-menu" id="close-menu">
                        <span class="close-first"></span>
                        <span class="close-second"></span>
                    </div>
                </div>
                <ul class="mobile-nav-links">
                    <li><a href="portfolio.html">Projects</a></li>
                    <li><a href="https://scholar.google.com/citations?user=A0RqAUgAAAAJ&hl=en&oi=sra">Publications</a></li>
                    <li><a href="https://1drv.ms/w/c/573fe21775d93be1/EdEFwUW9jwpAnPF5gp_8Zo0BMBXjoU_v5qz4fY5dTWffcg?e=dsuyWW">Curriculum Vitae</a></li>
                    <li><a href="about.html">Bio</a></li>
                </ul>
            </div>
        </div>
        <!--header-->
        <div class="portfolio-simple">
            <div class="portfolio-header">
                <h1>Projects</h1>
                <p>Throughout my career, I've developed robotic systems for deployment across space, oil & gas, chemical, defense, nuclear, food service, and construction industries. My work has focused on the development of advanced software and hardware solutions that work reliably in real-world environments where precision and dependability are critical. My expertise spans autonomous ground and aerial vehicle platform development, computer vision and deep learning algorithm development, mixed/augmented/virutal reality application development, and human-robot interaction research. Below are some of the key projects that showcase my work, each demonstrating how intelligent robotics can solve complex challenges in high-stakes hazardous operational settings.
                </p>
            </div>
            <div class="projects-list">
                <!-- Project 1 -->
                <div class="project-item">
                    <div class="project-image">
                        <img src="images/portfolio/gesture_example.jpg" alt="EgoNRG: Egocentric Navigation Robot Gesture Dataset">
                        <!-- <div class="project-category"></div> -->
                    </div>
                    <div class="project-content">
                        <h3>EgoNRG: Egocentric Navigation Robot Gesture Dataset</h3>
                        <p>Developed a comprehensive gesture recognition dataset for human-robot interaction in industrial, military, and emergency response environments. Created 3,000 first-person multi-view video recordings and 160,000 annotated images using four head-mounted cameras to capture navigation gestures while operators wear Personal Protective Equipment (PPE). The dataset includes twelve navigation gestures (ten Army Field Manual-derived, one deictic, one emblem) performed by 32 participants across indoor/outdoor environments with varying clothing and background conditions. Provides joint hand-arm segmentation labels for both covered and uncovered limbs, addressing critical limitations in existing egocentric gesture recognition systems for real-world deployment in challenging operational environments.</p>
                        <!-- <span class="project-publication">ACM/IEEE International Conference on Human-Robot Interaction (HRI '24)</span> -->
                        <div class="project-links">
                            <a href="https://utnuclearroboticspublic.github.io/egonrg-website/" class="project-link">Project Page</a>
                            <a href="" class="project-link">Paper (Coming Soon)</a>
                            <a href="https://github.com/UTNuclearRobotics/EgoNRG" class="project-link">Code</a>
                            <a href="https://dataverse.tdl.org/dataset.xhtml?persistentId=doi:10.18738/T8/DC4J0Q" class="project-link">Dataset</a>
                        </div>
                    </div>
                </div>
                <div class="project-item">
                    <div class="project-image">
                        <video autoplay muted loop playsinline>
                            <source src="videos/porfolio/voxel_structure_reconfigured.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <!-- <div class="project-category"></div> -->
                    </div>
                    <div class="project-content">
                        <h3>Multi-Robot Optimal Path Planning Control Algorithm for Space Structure Assembly</h3>
                        <p>New state space representations have been explored for multi-agent robotic systems, using A* and other algorithms to plan steps for robots to build structures off a CubeSat.The aim is to reconceptualize the roles and objectives of individual ARMADAS robots to enhance coordinated team effectiveness through simplified robot design using COTS parts with reduced motion complexity. Instead of walking around structures, 4 DoF robots will reside on the structure, using a bucket-brigade technique to transport voxels. For fastening, multiple, 3 DoF robots will move along a specific axis inside the structure, shifting complexity to the planning algorithms.
                            </p>
                        <div class="project-links">
                            <a href="" class="project-link">Project Page (Coming Soon)</a>
                            <a href="https://www.nasa.gov/automated-reconfigurable-mission-adaptive-digital-assembly-systems-armadas/" class="project-link">NASA | Project ARMADAS</a>
                            <a href="https://www.science.org/doi/10.1126/scirobotics.adi2746" class="project-link">ARMADAS Publication</a>
                        </div>
                    </div>
                </div>
                <div class="project-item">
                    <div class="project-image">
                        <video autoplay muted loop playsinline>
                            <source src="videos/porfolio/super_mega_demo_smashed.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div class="project-content">
                        <h3>AR-Affordances: Obtaining Affordance Primitives from Single Demonstration to Define Manipulation Contact Tasks</h3>
                        <p>This robot agnostic Learning from Demonstration (LfD) technique enables users to perform a single demonstration to define a manipulation contact-track for mobile manipulators in unstructured environments. Using a model-based, Affordance Primitive approach, mobile manipulators can navigate, plan, and execute a manipulation (e.g. turn a wheel valve) in any direction with any angle, outside the provided single demonstration trajectory.</p>
                        <div class="project-links">
                            <a href="https://utnuclearroboticspublic.github.io/ar-affordances/" class="project-link">Project Page</a>
                            <a href="https://ieeexplore.ieee.org/document/10342493/" class="project-link">Paper</a>
                            <a href="https://github.com/UTNuclearRoboticsPublic/ar-affordances" class="project-link">Code</a>
                        </div>
                    </div>
                </div>
                <div class="project-item">
                    <div class="project-image">
                        <!-- <video autoplay muted loop playsinline>
                            <source src="videos/porfolio/hipless_mmic.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video> -->
                        <div class="project-image">
                            <img src="images/portfolio/hipless_mmic.jpeg" alt="Hipless MMIC: Minimal Degree-Of-Freedom Voxel Bolting Robot for in-Space Structure Assembly">
                            <!-- <div class="project-category"></div> -->
                        </div>
                        <!-- <div class="project-category"></div> -->
                    </div>
                    <div class="project-content">
                        <h3>Hipless MMIC: Minimal Degree-Of-Freedom Voxel Bolting Robot for in-Space Structure Assembly</h3>
                        <p>Developed a functioning prototype robot for NASA's Project ARMADAS that was a less complex version of the original MMIC-I bolting robot. The idea was to reduce the cost and complexity of the original space assembly robot while still being able to perform the same tasks, to reduce mission failure rates and decrease costs.</p>
                        <div class="project-links">
                            <a href="" class="project-link">Project Page (Coming Soon)</a>
                            <a href="https://www.nasa.gov/automated-reconfigurable-mission-adaptive-digital-assembly-systems-armadas/" class="project-link">NASA | Project ARMADAS</a>
                            <a href="https://www.science.org/doi/10.1126/scirobotics.adi2746" class="project-link">MMIC-I Publication</a>
                        </div>
                    </div>
                </div>
                <div class="project-item">
                    <div class="project-image">
                        <video autoplay muted loop playsinline>
                            <source src="videos/porfolio/ar_star_interactions_cropped.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <!-- <div class="project-category"></div> -->
                    </div>
                    <div class="project-content">
                        <h3>AR-STAR: An Augmented Reality Tool for Online Modification of Robot Point Cloud Data</h3>
                        <p>During real-world robot deployments, sensor data often contains uncertainties that can compromise mission success, yet autonomous systems struggle to identify and resolve these issues on their own. To address this challenge, we created an augmented reality interface that enables human supervisors to view the robot's predictions in real-time and make corrections through three distinct interaction methods, allowing us to investigate which approaches users find most effective and intuitive.</p>
                        <!-- <span class="project-publication">ACM/IEEE International Conference on Human-Robot Interaction (HRI '24)</span> -->
                        <div class="project-links">
                            <a href="https://utnuclearroboticspublic.github.io/ar-star/" class="project-link">Project Page</a>
                            <a href="https://dl.acm.org/doi/10.1145/3610978.3640571" class="project-link">Paper</a>
                            <a href="https://github.com/UTNuclearRobotics/ARStarUnreal" class="project-link">Code</a>
                        </div>
                    </div>
                </div>
                <div class="project-item">
                    <div class="project-image">
                        <video autoplay muted loop playsinline>
                            <source src="videos/porfolio/small_sat_armadas_conops.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <!-- <div class="project-category"></div> -->
                    </div>
                    <div class="project-content">
                        <h3>Space Mission CubeSat ConOps Design for Assembly and Reconfiguration of Mechanical Metamaterials</h3>
                        <p>NASA's ARMADAS project develops autonomous robots that build large-scale structures in space using modular lattice blocks, enabling construction of lunar bases, Martian shelters, and communication infrastructure. This work presents a scaled-down 27U CubeSat mission designed to demonstrate the system's autonomous assembly capabilities in orbit and advance its Technology Readiness Level for future deployment.
                            </p>
                        <div class="project-links">
                            <a href="https://ieeexplore.ieee.org/abstract/document/11068443" class="project-link">Paper</a>
                            <a href="https://www.nasa.gov/automated-reconfigurable-mission-adaptive-digital-assembly-systems-armadas/" class="project-link">NASA | Project ARMADAS</a>
                        </div>
                    </div>
                </div>
                <div class="project-item">
                    <div class="project-image">
                        <video autoplay muted loop playsinline>
                            <source src="videos/porfolio/augre_xr_robotics_vid.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div class="project-content">
                        <h3>AugRE: HRI for Large Multi-Agent Teams</h3>
                        <p>Command, control, and supervise heterogeneous teams of robots using an augmented reality headset. Scalable with 50+ autonomous agents, this AR-based Human Robot Interaction (HRI) framework enables users to easily localize, supervise, and command a heterogeneous fleet of robots using an augmented reality headset. This work has been published and won second place at the Horizons at Extended Robotics IROS 2023 Workshop!</p>
                        <div class="project-links">
                            <a href="https://utnuclearroboticspublic.github.io/Augmented-Robot-Environment/" class="project-link">Projcet Page</a>
                            <a href="https://doi.org/10.1109/RO-MAN53752.2022.9900721" class="project-link">Paper</a>
                            <a href="https://github.com/UTNuclearRoboticsPublic/Augmented-Robot-Environment" class="project-link">Code</a>
                            <!-- <span class="project-publication">IEEE RO-MAN 2022 Publication</span> -->
                        </div>
                    </div>
                </div>

                <!-- <div class="project-item">
                    <div class="project-image">
                        <video autoplay muted loop playsinline>
                            <source src="videos/porfolio/augre_xr_robotics_vid.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div class="project-content">
                        <h3>Object Detection, Tracking, and Localization for Drones</h3>
                        <p></p>
                        <div class="project-links">
                            <a href="https://utnuclearroboticspublic.github.io/Augmented-Robot-Environment/" class="project-link">Projcet Page</a>
                            <a href="https://doi.org/10.1109/RO-MAN53752.2022.9900721" class="project-link">Paper</a>
                            <a href="https://github.com/UTNuclearRoboticsPublic/Augmented-Robot-Environment" class="project-link">Code</a>

                        </div>
                    </div>
                </div> -->

                <!-- Project 4 -->
                <div class="project-item">
                    <div class="project-image">
                        <!-- <img src="images/portfolio/haptics_project/haptics_cover_photo.jpg" alt="Rapid Item Identification w/ Haptics"> -->
                        <img src="images/blank_image.jpg" alt="Rapid Item Identification w/ Haptics">
                        <div class="project-category">Coming Soon</div>
                    </div>
                    <div class="project-content">
                        <h3>Rapid Item Identification w/ Haptics</h3>
                        <p>For UT Austin's ME 397 Haptics and Teleoperated Systems class with Dr. Ann Fey, my team and I developed a vibro-tactile haptic device that mounts to the HoloLens 2. The device provided sequential vibrations to the users head to help locate robots and other items of interest faster in their environment.</p>
                        <div class="project-links">
                            <a href="https://drive.google.com/file/d/1r8UXWziQ_Ku1-LZGoyn3_m7jvFbt2HFR/view?usp=sharing" class="project-link">Learn More</a>
                            <!-- <span class="project-publication">Unreal Engine 4, C++, Raspberry-Pi, HoloLens 2</span> -->
                        </div>
                    </div>
                </div>

                <!-- Project 5 -->
                <div class="project-item">
                    <div class="project-image">
                        <!-- <img src="images/portfolio/baxter_ar/kinova_ar_teleop_2.jpg" alt="Remote Teleoperation via AR-HMD"> -->
                        <img src="images/blank_image.jpg" alt="HRI for Large Multi-Agent Teams">
                        <div class="project-category">Coming Soon</div>
                    </div>
                    <div class="project-content">
                        <h3>Remote Teleoperation via AR-HMD</h3>
                        <p>This project strives to develop new methods with augmented reality devices to allow operators to confidently and effortlessly control dual-arm remote mobile robotic manipulators.</p>
                        <div class="project-links">
                            <a href="portfolio-pages/glove_box_ar.html" class="project-link">Learn More</a>
                            <!-- <span class="project-publication">IEEE IROS 2022 XR-ROB 2nd Place Prize</span> -->
                        </div>
                    </div>
                </div>

                <!-- Project 6 -->
                <div class="project-item">
                    <div class="project-image">
                        <!-- <img src="images/portfolio/auto_robots/auto_robots_small.jpg" alt="Self Driving Car (Full SLAM Dev)"> -->
                        <img src="images/blank_image.jpg" alt="Self Driving Car (Full SLAM Dev)">
                        <div class="project-category">Coming Soon</div>
                    </div>
                    <div class="project-content">
                        <h3>Self Driving Car (Full SLAM Dev)</h3>
                        <p>For UT Austin's CS 393R Autonomous Robots class with Dr. Joydeep Biswas, my teammate and I created a complete nav-stack for autonomous navigation of an Ackermann steering car. Localization performed with a custom 2D-LiDAR particle filter and a custom RRT algorithm was created for planning.</p>
                        <div class="project-links">
                            <a href="https://github.com/frank-Regal/cs393r_starter" class="project-link">Learn More</a>
                            <!-- <span class="project-publication">Custom SLAM & RRT Implementation</span> -->
                        </div>
                    </div>
                </div>

                <!-- Project 7 -->
                <div class="project-item">
                    <div class="project-image">
                        <!-- <img src="images/portfolio/kilnbot/kilnbot_cover_small.jpg" alt="Magnetic Inspection Robot"> -->
                        <img src="images/blank_image.jpg" alt="Magnetic Inspection Robot">
                        <div class="project-category">Coming Soon</div>
                    </div>
                    <div class="project-content">
                        <h3>Magnetic Inspection Robot (Full System Dev)</h3>
                        <p>Tasked with creating an alternative method to inspect boiler tube walls for corrosion to eliminate the need of a human to enter into a hazardous environment, I developed a modular, wirelessly controlled, 3D printed, magnetic inspection crawler from the ground up. Chain driven by two DC motors, the crawler was able to crawl up and down the 120 ft tall vertical boiler walls.</p>
                        <div class="project-links">
                            <a href="https://youtu.be/XDfSmJNMcdY" class="project-link">Learn More</a>
                            <!-- <span class="project-publication">$2,000 Total R&D Cost</span> -->
                        </div>
                    </div>
                </div>

                <!-- Project 8 -->
                <div class="project-item">
                    <div class="project-image">
                        <!-- <img src="images/portfolio/asbr_robots/asbr_cover_small.jpg" alt="Motion Planning & Control"> -->
                        <img src="images/blank_image.jpg" alt="Motion Planning & Control">
                        <div class="project-category">Coming Soon</div>
                    </div>
                    <div class="project-content">
                        <h3>Motion Planning & Control</h3>
                        <p>For UT Austin's ME 397 Algorithms for Sensor Based Robots class with Dr. Farshid Alambeigi, my teammate and I created algorithms to control a Kuka Quantec 6DOF robotic manipulator. We used screw theory to model and control the arm, developing impedance, admittance, hand-to-eye calibration, virtual fixtures, and point-cloud registration algorithms from scratch.</p>
                        <div class="project-links">
                            <a href="https://docs.google.com/presentation/d/1lDvpG_wKVkKxDGf0cAsa3iIihvhOZtIM/edit?usp=sharing&ouid=101661046954650984360&rtpof=true&sd=true" class="project-link">Learn More</a>
                            <!-- <span class="project-publication">Kuka Quantec 6DOF Control</span> -->
                        </div>
                    </div>
                </div>

                <!-- Project 9 -->
                <div class="project-item">
                    <div class="project-image">
                        <!-- <img src="images/portfolio/quick_release/quick_release.jpg" alt="Quick Release Four-Bar Mechanism"> -->
                        <img src="images/blank_image.jpg" alt="Quick Release Four-Bar Mechanism">
                        <div class="project-category">Coming Soon</div>
                    </div>
                    <div class="project-content">
                        <h3>Quick Release Four-Bar Mechanism</h3>
                        <p>The first prototype of this Quick Release Mechanism was designed to aid researchers in the computer science department at UT Austin attempting to perform the water bottle flip challenge with a robotic arm. My partner and I worked to develop a linear translation mechanism that can slowly close (clockwise drive) and rapidly open (counter-clockwise drive) via a DC motor.</p>
                        <div class="project-links">
                            <a href="portfolio-pages/quick_release.html" class="project-link">Learn More</a>

                        </div>
                    </div>
                </div>

                <!-- Project 10 -->
                <div class="project-item">
                    <div class="project-image">
                        <!-- <img src="images/portfolio/digital_control/digital_control_cover_small.jpg" alt="Robotic Latency Compensation"> -->
                        <img src="images/blank_image.jpg" alt="Robotic Latency Compensation">
                        <div class="project-category">Coming Soon</div>
                    </div>
                    <div class="project-content">
                        <h3>Robotic Latency Compensation</h3>
                        <p>For UT Austin's ME 397 Digital Controls class with Dr. Dongmei "Maggie" Chen, my team and I created a predictive digital control to compensate for latency found in a specific AR human-robot teaming application (AugRE). We used advanced control theory including lead compensators, pole placement, linear quadratic regulators (LQR), and Smith predictors.</p>
                        <div class="project-links">
                            <a href="https://drive.google.com/file/d/1r2AIZJUsWhH48wvF9f3dn0MFkfdKZQre/view?usp=sharing" class="project-link">Learn More</a>
                            <!-- <span class="project-publication">LQR & Smith Predictor Control</span> -->
                        </div>
                    </div>
                </div>

                <!-- Project 11 -->
                <div class="project-item">
                    <div class="project-image">
                        <!-- <img src="images/portfolio/dracos/dracos_cover_small.jpg" alt="AR Scanning and Mapping"> -->
                        <img src="images/blank_image.jpg" alt="AR Scanning and Mapping">
                        <div class="project-category">Coming Soon</div>
                    </div>
                    <div class="project-content">
                        <h3>AR Scanning and Mapping</h3>
                        <p>For my capstone at Drexel University, my team and I developed an AR tool to help construction workers easily map, scan, and superimpose building models on a spatially mapped environment. The DRACOS system integrated a HoloLens 1 AR device with a custom DJI F450 drone.</p>
                        <div class="project-links">
                            <a href="https://drive.google.com/file/d/1rarkUE_f2UhcwClF42Z5uRYrRQL4c59B/view?usp=sharing" class="project-link">Learn More</a>
                            <!-- <span class="project-publication">HoloLens 1 & DJI F450 Integration</span> -->
                        </div>
                    </div>
                </div>

                <!-- Project 12 -->
                <div class="project-item">
                    <div class="project-image">
                        <!-- <img src="images/portfolio/shadow_arm/robotic_shadow_arm.jpg" alt="Teleoperation via Shadow Arm"> -->
                        <img src="images/blank_image.jpg" alt="Teleoperation via Shadow Arm">
                        <div class="project-category">Coming Soon</div>
                    </div>
                    <div class="project-content">
                        <h3>Teleoperation via Shadow Arm</h3>
                        <p>For Drexel University's MEM 455 Robotics class with Dr. James Tangorra, I developed a scaled down 5R robotic manipulator that was tasked to perform a simulated disaster recovery clean-up effort. To control the robot I created a 3D printed shadow arm that mapped potentiometer readings at each joint to physical joint positions of the real arm.</p>
                        <div class="project-links">
                            <a href="https://drive.google.com/file/d/1svQTxcwqArXjck7zESCHhT93I6uJxzH7/view?usp=sharing" class="project-link">Learn More</a>
                            <!-- <span class="project-publication">5R Manipulator & 3D Printed Control</span> -->
                        </div>
                    </div>
                </div>
            </div>
                <!-- <div class="footer">
                    <div class="footer-text">
                        Last Updated September 2023 by Frank Regal
                        <span class="footer-image">
                            Made In America<img src="images/united-states.png" height="24px">
                        </span> 
                    </div>
                </div> -->
        </div><!--blog--end-->

    <!--division scripts-->
    <script src="js/jquery.min.js"></script>
    <script src="js/particles.js"></script>
    <script src="js/particles.min.js"></script>
    <script src="js/index.js"></script>
    <!--divisiion scripts-->

    </body>
</html>